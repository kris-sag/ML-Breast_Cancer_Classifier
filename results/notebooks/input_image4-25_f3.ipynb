{"cells":[{"cell_type":"code","execution_count":1,"source":["from __future__ import print_function, division\n","from this import d\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import copy\n","from PIL import Image\n","\n","\n","#repo_dir = 'C:\\\\Users\\MP_lab_GPU\\Desktop\\Senior Design 2019\\Senior Design\\'\n","\n","# CHANGE THIS DIRECTORY TO THE ML BREAST CANCER TOTAL FILES FOLDER\n","# repo_dir = r'C:\\Users\\joekh\\Documents\\GitHub\\ML-Breat_Cancer_Classfier\\\\'\n","reop_dir = r'C:\\Users\\Kris\\..vs code files\\senior design\\ML-Breat_Cancer_Classfier-master'"],"outputs":[{"output_type":"stream","name":"stdout","text":["The Zen of Python, by Tim Peters\n","\n","Beautiful is better than ugly.\n","Explicit is better than implicit.\n","Simple is better than complex.\n","Complex is better than complicated.\n","Flat is better than nested.\n","Sparse is better than dense.\n","Readability counts.\n","Special cases aren't special enough to break the rules.\n","Although practicality beats purity.\n","Errors should never pass silently.\n","Unless explicitly silenced.\n","In the face of ambiguity, refuse the temptation to guess.\n","There should be one-- and preferably only one --obvious way to do it.\n","Although that way may not be obvious at first unless you're Dutch.\n","Now is better than never.\n","Although never is often better than *right* now.\n","If the implementation is hard to explain, it's a bad idea.\n","If the implementation is easy to explain, it may be a good idea.\n","Namespaces are one honking great idea -- let's do more of those!\n"]}],"metadata":{}},{"cell_type":"code","execution_count":2,"source":["imsize = 256\n","loader = transforms.Compose([\n","            # transforms.RandomResizedCrop(224),\n","            transforms.Resize(256),\n","            transforms.CenterCrop(224),\n","            transforms.RandomResizedCrop(224),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        ])\n","\n","#\n","def image_loader(loader, image_name):\n","    image = Image.open(image_name)\n","    image = loader(image).float()\n","    image = torch.tensor(image, requires_grad=True)\n","    image = image.unsqueeze(0)\n","    return image\n","\n","# model = models.resnet152(pretrained=True)\n","# num_ftrs = model.fc.in_features\n","# model.fc = nn.Linear(num_ftrs, 2)\n","\n","model = models.vgg16(pretrained=True)\n","num_ftrs = model.classifier[0].in_features\n","model.classifier = nn.Linear(num_ftrs, 2)\n","\n","\n","#CHANGE PATH TO MODEL PATH, CHANGE MAP LOCATION BASED ON GPU\n","model.load_state_dict(torch.load(r'C:\\Users\\Kris\\..vs code files\\senior design\\ML-Breat_Cancer_Classfier-master\\model\\\\4-25_vgg_model_state_dict-Fold3.pt',\n","                                 map_location=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")), strict=False)\n","model.eval()\n","#CHANGE PATH LOCATION TO FOLDER WITH CROPPED MALIGNANT IMAGES\n","malignant_path = r'C:\\Users\\Kris\\..vs code files\\senior design\\ML-Breat_Cancer_Classfier-master\\images\\Photos for Testing\\data_test\\Cancer'\n","#CHANGE PATH LOCATION TO FOLDER WITH CROPPED NONMALIGNANT IMAGES\n","non_malignant_path = r'C:\\Users\\Kris\\..vs code files\\senior design\\ML-Breat_Cancer_Classfier-master\\images\\Photos for Testing\\data_test\\Not_Cancer'"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":3,"source":["#### THE FIRST ARGUMENT IS BENIGN, SECOND IS MALIGNANT\n","tp = [] #num correctly diagnosed malignant\n","fp = [] #num incorrectly diagnosed malignant\n","tn = [] #num correctly diagnosed benign\n","fn = [] #num incorrectly diagnosed negative\n","errors = 0\n","num_malignant = 0\n","num_non_malignant = 0\n","\n","for i in os.listdir(malignant_path):\n","    try:\n","        image = image_loader(loader, os.path.join(malignant_path, i))\n","        \n","        y = model(image)\n","        if y.argmax().item() == 0:\n","            fn.append(i)\n","        else:\n","            tp.append(i)\n","        num_malignant +=1\n","    except:\n","        print(i)\n","        errors +=1\n","        \n","for i in os.listdir(non_malignant_path):\n","    try:\n","        image = image_loader(loader, os.path.join(non_malignant_path, i))\n","        num_non_malignant +=1\n","        y = model(image)\n","        if y.argmax().item() == 0:\n","            tn.append(i)\n","        else:\n","            fp.append(i)\n","    except:\n","        errors +=1\n","\n","print(\"True Positives: \" + str(len(tp)))\n","print(\"False Negatives: \"+ str(len(fn)))\n","print(\"True Negatives: \"+ str(len(tn)))\n","print(\"False Positives: \"+ str(len(fp)))\n","print(\"Total Number of images: \"+ str((len(tp)+len(tn)+len(fp)+len(fn))))"],"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-2-790358f23313>:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  image = torch.tensor(image, requires_grad=True)\n"]},{"output_type":"stream","name":"stdout","text":["True Positives: 84\n","False Negatives: 790\n","True Negatives: 270\n","False Positives: 85\n","Total Number of images: 1229\n"]}],"metadata":{}}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}